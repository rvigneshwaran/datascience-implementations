{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import optimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimization Result:\n",
      "  message: Optimization terminated successfully.\n",
      "  success: True\n",
      "   status: 0\n",
      "      fun: 0.0\n",
      "        x: [-1.000e+00]\n",
      "      nit: 2\n",
      "      jac: [ 0.000e+00]\n",
      " hess_inv: [[ 5.000e-01]]\n",
      "     nfev: 6\n",
      "     njev: 3\n"
     ]
    }
   ],
   "source": [
    "# Example 1: Minimization of a Simple Function\n",
    "def func(x):\n",
    "    return x**2 + 2*x + 1\n",
    "\n",
    "result_minimize = optimize.minimize(func, x0=0)\n",
    "print(\"Minimization Result:\")\n",
    "print(result_minimize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimization Result with Linear Constraint:\n",
      " message: Optimization terminated successfully\n",
      " success: True\n",
      "  status: 0\n",
      "     fun: 0.5\n",
      "       x: [ 5.000e-01  5.000e-01]\n",
      "     nit: 3\n",
      "     jac: [ 1.000e+00  1.000e+00]\n",
      "    nfev: 10\n",
      "    njev: 3\n"
     ]
    }
   ],
   "source": [
    "# Example 2: Minimization with Constraints (Linear Constraints)\n",
    "def func_with_constraints(x):\n",
    "    return x[0]**2 + x[1]**2\n",
    "\n",
    "constraint_linear = {'type': 'eq', 'fun': lambda x: x[0] + x[1] - 1}\n",
    "result_minimize_constraints = optimize.minimize(func_with_constraints, x0=[0, 0], constraints=constraint_linear)\n",
    "print(\"Minimization Result with Linear Constraint:\")\n",
    "print(result_minimize_constraints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimization Result with Nonlinear Constraint:\n",
      " message: Optimization terminated successfully\n",
      " success: True\n",
      "  status: 0\n",
      "     fun: 0.0\n",
      "       x: [ 0.000e+00  0.000e+00]\n",
      "     nit: 1\n",
      "     jac: [ 1.490e-08  1.490e-08]\n",
      "    nfev: 3\n",
      "    njev: 1\n"
     ]
    }
   ],
   "source": [
    "# Example 3: Minimization with Constraints (Nonlinear Constraints)\n",
    "def func_with_nonlinear_constraints(x):\n",
    "    return x[0]**2 + x[1]**2\n",
    "\n",
    "constraint_nonlinear = {'type': 'ineq', 'fun': lambda x: x[0] - x[1]}\n",
    "result_minimize_nonlinear_constraints = optimize.minimize(func_with_nonlinear_constraints, x0=[0, 0], constraints=constraint_nonlinear)\n",
    "print(\"Minimization Result with Nonlinear Constraint:\")\n",
    "print(result_minimize_nonlinear_constraints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimization Result with Bounds:\n",
      "  message: CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL\n",
      "  success: True\n",
      "   status: 0\n",
      "      fun: 0.0\n",
      "        x: [ 0.000e+00  0.000e+00]\n",
      "      nit: 0\n",
      "      jac: [ 1.000e-08  1.000e-08]\n",
      "     nfev: 3\n",
      "     njev: 1\n",
      " hess_inv: <2x2 LbfgsInvHessProduct with dtype=float64>\n"
     ]
    }
   ],
   "source": [
    "# Example 4: Minimization with Bounds\n",
    "result_minimize_bounds = optimize.minimize(func_with_constraints, x0=[0, 0], bounds=((0, None), (0, None)))\n",
    "print(\"Minimization Result with Bounds:\")\n",
    "print(result_minimize_bounds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimization Result with Jac and Hessian Functions:\n",
      "  message: Optimization terminated successfully.\n",
      "  success: True\n",
      "   status: 0\n",
      "      fun: 0.0\n",
      "        x: [ 0.000e+00  0.000e+00]\n",
      "      nit: 0\n",
      "      jac: [ 0.000e+00  0.000e+00]\n",
      " hess_inv: [[1 0]\n",
      "            [0 1]]\n",
      "     nfev: 1\n",
      "     njev: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rvign\\AppData\\Roaming\\Python\\Python311\\site-packages\\scipy\\optimize\\_minimize.py:565: RuntimeWarning: Method BFGS does not use Hessian information (hess).\n",
      "  warn('Method %s does not use Hessian information (hess).' % method,\n"
     ]
    }
   ],
   "source": [
    "# Example 5: Minimization with Jac and Hessian Functions\n",
    "def func_with_jac_hess(x):\n",
    "    return x[0]**2 + x[1]**2\n",
    "\n",
    "def jac_func(x):\n",
    "    return [2*x[0], 2*x[1]]\n",
    "\n",
    "def hess_func(x):\n",
    "    return np.array([[2, 0], [0, 2]])\n",
    "\n",
    "result_minimize_jac_hess = optimize.minimize(func_with_jac_hess, x0=[0, 0], jac=jac_func, hess=hess_func)\n",
    "print(\"Minimization Result with Jac and Hessian Functions:\")\n",
    "print(result_minimize_jac_hess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global Optimization Result with Basinhopping:\n",
      "                    message: ['requested number of basinhopping iterations completed successfully']\n",
      "                    success: True\n",
      "                        fun: 0.0\n",
      "                          x: [-1.000e+00]\n",
      "                        nit: 100\n",
      "      minimization_failures: 0\n",
      "                       nfev: 708\n",
      "                       njev: 354\n",
      " lowest_optimization_result:  message: Optimization terminated successfully.\n",
      "                              success: True\n",
      "                               status: 0\n",
      "                                  fun: 0.0\n",
      "                                    x: [-1.000e+00]\n",
      "                                  nit: 2\n",
      "                                  jac: [ 0.000e+00]\n",
      "                             hess_inv: [[ 5.000e-01]]\n",
      "                                 nfev: 6\n",
      "                                 njev: 3\n"
     ]
    }
   ],
   "source": [
    "# Example 6: Global Optimization with Basinhopping\n",
    "def func_global(x):\n",
    "    return x**2 + 2*x + 1\n",
    "\n",
    "result_basinhopping = optimize.basinhopping(func_global, x0=0)\n",
    "print(\"Global Optimization Result with Basinhopping:\")\n",
    "print(result_basinhopping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global Optimization Result with Dual Annealing:\n",
      " message: ['Maximum number of iteration reached']\n",
      " success: True\n",
      "  status: 0\n",
      "     fun: 4.440892098500626e-16\n",
      "       x: [-1.000e+00]\n",
      "     nit: 1000\n",
      "    nfev: 2007\n",
      "    njev: 3\n",
      "    nhev: 0\n"
     ]
    }
   ],
   "source": [
    "# Example 7: Global Optimization with Dual Annealing\n",
    "result_dual_annealing = optimize.dual_annealing(func_global, bounds=[(-10, 10)])\n",
    "print(\"Global Optimization Result with Dual Annealing:\")\n",
    "print(result_dual_annealing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global Optimization Result with Differential Evolution:\n",
      " message: Optimization terminated successfully.\n",
      " success: True\n",
      "     fun: 0.0\n",
      "       x: [-1.000e+00]\n",
      "     nit: 31\n",
      "    nfev: 482\n"
     ]
    }
   ],
   "source": [
    "# Example 8: Global Optimization with Differential Evolution\n",
    "result_differential_evolution = optimize.differential_evolution(func_global, bounds=[(-10, 10)])\n",
    "print(\"Global Optimization Result with Differential Evolution:\")\n",
    "print(result_differential_evolution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Finding Result with Broyden's Method:\n",
      " message: The iteration is not making good progress, as measured by the \n",
      "            improvement from the last ten iterations.\n",
      " success: False\n",
      "  status: 5\n",
      "     fun: [-6.151e-01]\n",
      "       x: [-5.774e-01]\n",
      "    nfev: 22\n",
      "    fjac: [[-1.000e+00]]\n",
      "       r: [-3.601e-03]\n",
      "     qtf: [ 6.151e-01]\n"
     ]
    }
   ],
   "source": [
    "# Example 9: Root Finding with Broyden's Method\n",
    "def func_root(x):\n",
    "    return x**3 - x - 1\n",
    "\n",
    "result_broyden = optimize.root(func_root, x0=0)\n",
    "print(\"Root Finding Result with Broyden's Method:\")\n",
    "print(result_broyden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Finding Result with Newton-Raphson Method:\n",
      " message: Both actual and predicted relative reductions in the sum of squares\n",
      "            are at most 0.000000\n",
      " success: True\n",
      "  status: 1\n",
      "     fun: [-6.151e-01]\n",
      "       x: [-5.773e-01]\n",
      "   cov_x: [[ 3.395e+11]]\n",
      "    nfev: 18\n",
      "    fjac: [[ 1.716e-06]]\n",
      "    ipvt: [1]\n",
      "     qtf: [ 6.151e-01]\n"
     ]
    }
   ],
   "source": [
    "# Example 10: Root Finding with Newton-Raphson Method\n",
    "result_newton = optimize.root(func_root, x0=0, method='lm')\n",
    "print(\"Root Finding Result with Newton-Raphson Method:\")\n",
    "print(result_newton)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Finding Result with Anderson Mixing Method:\n",
      " message: The maximum number of iterations allowed has been reached.\n",
      " success: False\n",
      "  status: 2\n",
      "     fun: [-6.168e-01]\n",
      "       x: -0.545951755724848\n",
      "     nit: 200\n"
     ]
    }
   ],
   "source": [
    "# Example 11: Root Finding with Anderson Mixing Method\n",
    "result_anderson = optimize.root(func_root, x0=0, method='anderson')\n",
    "print(\"Root Finding Result with Anderson Mixing Method:\")\n",
    "print(result_anderson)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Least Squares Fitting Parameters:\n",
      "[0.99357586 2.14032406 2.56059344]\n"
     ]
    }
   ],
   "source": [
    "# Example 12: Least Squares Fitting with Curve Fit\n",
    "def func_curve_fit(x, a, b, c):\n",
    "    return a * x**2 + b * x + c\n",
    "\n",
    "x_data = np.linspace(0, 10, 100)\n",
    "y_data = func_curve_fit(x_data, 1, 2, 3) + np.random.normal(0, 1, 100)\n",
    "params, covariance = optimize.curve_fit(func_curve_fit, x_data, y_data)\n",
    "print(\"Least Squares Fitting Parameters:\")\n",
    "print(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Non-linear Least Squares Fitting Parameters with Levenberg-Marquardt:\n",
      "[0.99357586 2.14032406 2.56059344]\n"
     ]
    }
   ],
   "source": [
    "# Example 13: Non-linear Least Squares Fitting with Levenberg-Marquardt\n",
    "params_lm, _ = optimize.curve_fit(func_curve_fit, x_data, y_data, method='lm')\n",
    "print(\"Non-linear Least Squares Fitting Parameters with Levenberg-Marquardt:\")\n",
    "print(params_lm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Non-linear Least Squares Fitting Parameters with Trust Region Reflective Method:\n",
      "[0.99357586 2.14032407 2.5605934 ]\n"
     ]
    }
   ],
   "source": [
    "# Example 14: Non-linear Least Squares Fitting with Trust Region Reflective Method\n",
    "params_trf, _ = optimize.curve_fit(func_curve_fit, x_data, y_data, method='trf')\n",
    "print(\"Non-linear Least Squares Fitting Parameters with Trust Region Reflective Method:\")\n",
    "print(params_trf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global Minimum Result with Shgo:\n",
      " message: Optimization terminated successfully.\n",
      " success: True\n",
      "     fun: 0.0\n",
      "    funl: [ 0.000e+00]\n",
      "       x: [ 0.000e+00  0.000e+00]\n",
      "      xl: [[ 0.000e+00  0.000e+00]]\n",
      "     nit: 1\n",
      "    nfev: 8\n",
      "   nlfev: 3\n",
      "   nljev: 1\n",
      "   nlhev: 0\n"
     ]
    }
   ],
   "source": [
    "# Example 15: Finding Global Minimum with Shgo\n",
    "def func_global_min(x):\n",
    "    return x[0]**2 + x[1]**2\n",
    "\n",
    "result_shgo = optimize.shgo(func_global_min, bounds=[(-10, 10), (-10, 10)])\n",
    "print(\"Global Minimum Result with Shgo:\")\n",
    "print(result_shgo)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global Minimum Result with Dual Annealing:\n",
      " message: ['Maximum number of iteration reached']\n",
      " success: True\n",
      "  status: 0\n",
      "     fun: 3.484161118818652e-14\n",
      "       x: [ 1.407e-07 -1.226e-07]\n",
      "     nit: 1000\n",
      "    nfev: 4010\n",
      "    njev: 3\n",
      "    nhev: 0\n"
     ]
    }
   ],
   "source": [
    "# Example 16: Finding Global Minimum with Dual Annealing\n",
    "result_dual_annealing_min = optimize.dual_annealing(func_global_min, bounds=[(-10, 10), (-10, 10)])\n",
    "print(\"Global Minimum Result with Dual Annealing:\")\n",
    "print(result_dual_annealing_min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global Minimum Result with Monte Carlo:\n",
      " message: ['Maximum number of iteration reached']\n",
      " success: True\n",
      "  status: 0\n",
      "     fun: 4.91929321938722e-17\n",
      "       x: [-4.968e-09 -4.951e-09]\n",
      "     nit: 1000\n",
      "    nfev: 4013\n",
      "    njev: 4\n",
      "    nhev: 0\n"
     ]
    }
   ],
   "source": [
    "# Example 17: Finding Global Minimum with Monte Carlo\n",
    "result_mc = optimize.dual_annealing(func_global_min, bounds=[(-10, 10), (-10, 10)])\n",
    "print(\"Global Minimum Result with Monte Carlo:\")\n",
    "print(result_mc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Programming Result with Linprog:\n",
      "        message: Optimization terminated successfully. (HiGHS Status 7: Optimal)\n",
      "        success: True\n",
      "         status: 0\n",
      "            fun: -18.0\n",
      "              x: [ 0.000e+00  9.000e+00]\n",
      "            nit: 0\n",
      "          lower:  residual: [ 0.000e+00  9.000e+00]\n",
      "                 marginals: [ 5.000e+00  0.000e+00]\n",
      "          upper:  residual: [       inf        inf]\n",
      "                 marginals: [ 0.000e+00  0.000e+00]\n",
      "          eqlin:  residual: []\n",
      "                 marginals: []\n",
      "        ineqlin:  residual: [ 1.200e+01  0.000e+00]\n",
      "                 marginals: [-0.000e+00 -2.000e+00]\n",
      " mip_node_count: 0\n",
      " mip_dual_bound: 0.0\n",
      "        mip_gap: 0.0\n"
     ]
    }
   ],
   "source": [
    "# Example 18: Linear Programming with Linprog\n",
    "c = [-1, -2]\n",
    "A = [[1, -1], [3, 1]]\n",
    "b = [3, 9]\n",
    "result_linprog = optimize.linprog(c, A_ub=A, b_ub=b)\n",
    "print(\"Linear Programming Result with Linprog:\")\n",
    "print(result_linprog)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Programming Result with Simplex Method:\n",
      " message: Optimization terminated successfully.\n",
      " success: True\n",
      "  status: 0\n",
      "     fun: -18.0\n",
      "       x: [ 0.000e+00  9.000e+00]\n",
      "     nit: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rvign\\AppData\\Local\\Temp\\ipykernel_14828\\1735413238.py:2: DeprecationWarning: `method='simplex'` is deprecated and will be removed in SciPy 1.11.0. Please use one of the HiGHS solvers (e.g. `method='highs'`) in new code.\n",
      "  result_simplex = optimize.linprog(c, A_ub=A, b_ub=b, method='simplex')\n"
     ]
    }
   ],
   "source": [
    "# Example 19: Linear Programming with Simplex Method\n",
    "result_simplex = optimize.linprog(c, A_ub=A, b_ub=b, method='simplex')\n",
    "print(\"Linear Programming Result with Simplex Method:\")\n",
    "print(result_simplex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Convex Optimization Result with minimize_scalar:\n",
      " message: \n",
      "          Optimization terminated successfully;\n",
      "          The returned value satisfies the termination criteria\n",
      "          (using xtol = 1.48e-08 )\n",
      " success: True\n",
      "     fun: 0.0\n",
      "       x: 2.0\n",
      "     nit: 4\n",
      "    nfev: 8\n"
     ]
    }
   ],
   "source": [
    "# Example 20: Convex Optimization with Scipy Optimize\n",
    "from scipy.optimize import minimize_scalar\n",
    "\n",
    "def func_convex(x):\n",
    "    return x**2 - 4*x + 4\n",
    "\n",
    "result_convex = minimize_scalar(func_convex)\n",
    "print(\"Convex Optimization Result with minimize_scalar:\")\n",
    "print(result_convex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Constrained Convex Optimization Result with minimize_scalar:\n",
      " message: Solution found.\n",
      " success: True\n",
      "  status: 0\n",
      "     fun: 0.0\n",
      "       x: 1.9999999999999998\n",
      "     nit: 6\n",
      "    nfev: 6\n"
     ]
    }
   ],
   "source": [
    "# Example 21: Constrained Convex Optimization with minimize_scalar\n",
    "def func_constrained(x):\n",
    "    return x**2 - 4*x + 4\n",
    "\n",
    "result_constrained = minimize_scalar(func_constrained, bounds=(0, 5), method='bounded')\n",
    "print(\"Constrained Convex Optimization Result with minimize_scalar:\")\n",
    "print(result_constrained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Roots of Non-linear Equations with Root:\n",
      " message: The solution converged.\n",
      " success: True\n",
      "  status: 1\n",
      "     fun: [-1.776e-15  0.000e+00]\n",
      "       x: [ 1.414e+00  1.414e+00]\n",
      "    nfev: 9\n",
      "    fjac: [[-9.428e-01 -3.333e-01]\n",
      "           [ 3.333e-01 -9.428e-01]]\n",
      "       r: [-3.000e+00 -2.333e+00  1.886e+00]\n",
      "     qtf: [ 1.684e-09 -5.954e-10]\n"
     ]
    }
   ],
   "source": [
    "# Example 22: Finding Roots of Non-linear Equations with Root\n",
    "def func_root_nonlinear(x):\n",
    "    return [x[0]**2 + x[1]**2 - 4, x[0] - x[1]]\n",
    "\n",
    "result_root_nonlinear = optimize.root(func_root_nonlinear, x0=[1, 1])\n",
    "print(\"Roots of Non-linear Equations with Root:\")\n",
    "print(result_root_nonlinear)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Local Minimum Result with Nelder-Mead:\n",
      "       message: Optimization terminated successfully.\n",
      "       success: True\n",
      "        status: 0\n",
      "           fun: 0.0\n",
      "             x: [-1.000e+00]\n",
      "           nit: 25\n",
      "          nfev: 50\n",
      " final_simplex: (array([[-1.000e+00],\n",
      "                       [-9.999e-01]]), array([ 0.000e+00,  3.906e-09]))\n"
     ]
    }
   ],
   "source": [
    "# Example 23: Finding Local Minimum with Nelder-Mead\n",
    "result_nelder_mead = optimize.minimize(func, x0=0, method='Nelder-Mead')\n",
    "print(\"Local Minimum Result with Nelder-Mead:\")\n",
    "print(result_nelder_mead)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Local Minimum Result with CG:\n",
      " message: Optimization terminated successfully.\n",
      " success: True\n",
      "  status: 0\n",
      "     fun: 0.0\n",
      "       x: [-1.000e+00]\n",
      "     nit: 1\n",
      "     jac: [ 0.000e+00]\n",
      "    nfev: 6\n",
      "    njev: 3\n"
     ]
    }
   ],
   "source": [
    "# Example 24: Finding Local Minimum with CG\n",
    "result_cg = optimize.minimize(func, x0=0, method='CG')\n",
    "print(\"Local Minimum Result with CG:\")\n",
    "print(result_cg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Local Minimum Result with BFGS:\n",
      "  message: Optimization terminated successfully.\n",
      "  success: True\n",
      "   status: 0\n",
      "      fun: 0.0\n",
      "        x: [-1.000e+00]\n",
      "      nit: 2\n",
      "      jac: [ 0.000e+00]\n",
      " hess_inv: [[ 5.000e-01]]\n",
      "     nfev: 6\n",
      "     njev: 3\n"
     ]
    }
   ],
   "source": [
    "# Example 25: Finding Local Minimum with BFGS\n",
    "result_bfgs = optimize.minimize(func, x0=0, method='BFGS')\n",
    "print(\"Local Minimum Result with BFGS:\")\n",
    "print(result_bfgs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
